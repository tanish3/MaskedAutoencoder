{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops\n!pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:03.67397Z","iopub.execute_input":"2022-01-19T23:08:03.674487Z","iopub.status.idle":"2022-01-19T23:08:12.948122Z","shell.execute_reply.started":"2022-01-19T23:08:03.674446Z","shell.execute_reply":"2022-01-19T23:08:12.947331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n# from vit_pytorch import ViT, MAE\nfrom glob import glob\nfrom torch.utils.data import Dataset, DataLoader\nimport os\n# from skimage import io, transform\nfrom tqdm import tqdm_notebook as tqdm\nimport pandas as pd\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom matplotlib import pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom timm.models.vision_transformer import PatchEmbed, Block\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:21.581721Z","iopub.execute_input":"2022-01-19T23:08:21.581976Z","iopub.status.idle":"2022-01-19T23:08:28.628313Z","shell.execute_reply.started":"2022-01-19T23:08:21.581944Z","shell.execute_reply":"2022-01-19T23:08:28.62696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage import io\nim = io.imread('../input/neuron/train-input.tif')\nprint(im.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:28.638439Z","iopub.execute_input":"2022-01-19T23:08:28.638775Z","iopub.status.idle":"2022-01-19T23:08:31.901539Z","shell.execute_reply.started":"2022-01-19T23:08:28.638729Z","shell.execute_reply":"2022-01-19T23:08:31.899433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 2D SlicesDataGenerator","metadata":{}},{"cell_type":"code","source":"# train, val = train_test_split(image, test_size=0.2)\ntrain, val = im[0:80, :, :], im[80:, :, :]","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:31.982199Z","iopub.execute_input":"2022-01-19T23:08:31.982488Z","iopub.status.idle":"2022-01-19T23:08:31.988761Z","shell.execute_reply.started":"2022-01-19T23:08:31.982457Z","shell.execute_reply":"2022-01-19T23:08:31.987995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.046706Z","iopub.execute_input":"2022-01-19T23:08:32.046974Z","iopub.status.idle":"2022-01-19T23:08:32.107612Z","shell.execute_reply.started":"2022-01-19T23:08:32.046923Z","shell.execute_reply":"2022-01-19T23:08:32.106807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SliceDataLoader(Dataset):\n    def __init__(self,input_vol,size, transform=None):\n        \n        self.vol = input_vol\n        self.x, _, _ = input_vol.shape\n        self.length = size\n        self.transform = transforms.Compose([\n                                                transforms.ToTensor(),    \n                                           ])\n        self.randomlist = pd.DataFrame(np.concatenate((np.random.randint(0,768,size=(self.length, 2)), np.random.randint(0,self.x,size=(self.length, 1))), axis=1))\n        \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        x, y, z = self.randomlist.iloc[idx]\n        image = self.vol[z, x:x+256, y:y+256]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.109223Z","iopub.execute_input":"2022-01-19T23:08:32.109684Z","iopub.status.idle":"2022-01-19T23:08:32.119646Z","shell.execute_reply.started":"2022-01-19T23:08:32.109647Z","shell.execute_reply":"2022-01-19T23:08:32.118918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self,input_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.image_list = input_dir\n#         self.landmarks_frame = pd.read_csv(csv_file)\n#         self.root_dir = root_dir\n        self.transform = transforms.Compose([\n                                               transforms.ToTensor(),\n                                           ])\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.image_list.iloc[idx, 0])\n        \n        image = np.load(img_name)\n        image = cv2.resize(image, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n        if self.transform:\n            image = self.transform(image)\n            \n#         print(image.shape)\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.121085Z","iopub.execute_input":"2022-01-19T23:08:32.121827Z","iopub.status.idle":"2022-01-19T23:08:32.133216Z","shell.execute_reply.started":"2022-01-19T23:08:32.121786Z","shell.execute_reply":"2022-01-19T23:08:32.132435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # train_set = FaceLandmarksDataset(image)\n# train_set = FaceLandmarksDataset(train)\n# val_set = FaceLandmarksDataset(val)\n# # val_set = FaceLandmarksDataset(glob(\"../input/imagenetmini-1000/imagenet-mini/val/*/*.JPEG\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.134954Z","iopub.execute_input":"2022-01-19T23:08:32.135265Z","iopub.status.idle":"2022-01-19T23:08:32.145719Z","shell.execute_reply.started":"2022-01-19T23:08:32.135224Z","shell.execute_reply":"2022-01-19T23:08:32.144957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_set = FaceLandmarksDataset(image)\ntrain_set = SliceDataLoader(train, 500)\nval_set = SliceDataLoader(val, 100)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.147348Z","iopub.execute_input":"2022-01-19T23:08:32.147686Z","iopub.status.idle":"2022-01-19T23:08:32.155971Z","shell.execute_reply.started":"2022-01-19T23:08:32.147637Z","shell.execute_reply":"2022-01-19T23:08:32.15516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 2 #25\nWORKERS = 0\nSHUFFLE = True\ntrain_loader = DataLoader(train_set,  batch_size=BATCH_SIZE,\n                        shuffle=SHUFFLE, num_workers=WORKERS)\nval_loader = DataLoader(val_set,  batch_size=BATCH_SIZE,\n                        shuffle=SHUFFLE, num_workers=WORKERS)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.157651Z","iopub.execute_input":"2022-01-19T23:08:32.157926Z","iopub.status.idle":"2022-01-19T23:08:32.166728Z","shell.execute_reply.started":"2022-01-19T23:08:32.157891Z","shell.execute_reply":"2022-01-19T23:08:32.166034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out) # (M, D/2)\n    emb_cos = np.cos(out) # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.168261Z","iopub.execute_input":"2022-01-19T23:08:32.168671Z","iopub.status.idle":"2022-01-19T23:08:32.181107Z","shell.execute_reply.started":"2022-01-19T23:08:32.168635Z","shell.execute_reply":"2022-01-19T23:08:32.180426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskedAutoencoderViT(nn.Module):\n    \"\"\" Masked Autoencoder with VisionTransformer backbone\n    \"\"\"\n    def __init__(self, img_size=256, patch_size=16, in_chans=1,\n                 embed_dim=1024, depth=24, num_heads=16,\n                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n        super().__init__()\n\n        # --------------------------------------------------------------------------\n        # MAE encoder specifics\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n\n        self.blocks = nn.ModuleList([\n            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n        # --------------------------------------------------------------------------\n\n        # --------------------------------------------------------------------------\n        # MAE decoder specifics\n        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\n        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n\n        self.decoder_blocks = nn.ModuleList([\n            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n            for i in range(decoder_depth)])\n\n        self.decoder_norm = norm_layer(decoder_embed_dim)\n        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n        # --------------------------------------------------------------------------\n\n        self.norm_pix_loss = norm_pix_loss\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialization\n        # initialize (and freeze) pos_embed by sin-cos embedding\n        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n\n        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n        w = self.patch_embed.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.cls_token, std=.02)\n        torch.nn.init.normal_(self.mask_token, std=.02)\n\n        # initialize nn.Linear and nn.LayerNorm\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            # we use xavier_uniform following official JAX ViT:\n            torch.nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def patchify(self, imgs):\n        \"\"\"\n        imgs: (N, 3, H, W)\n        x: (N, L, patch_size**2 *3)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n\n        h = w = imgs.shape[2] // p\n        x = imgs.reshape(shape=(imgs.shape[0], 1, h, p, w, p))\n        x = torch.einsum('nchpwq->nhwpqc', x)\n        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 1))\n        return x\n\n    def unpatchify(self, x):\n        \"\"\"\n        x: (N, L, patch_size**2 *3)\n        imgs: (N, 3, H, W)\n        \"\"\"\n        p = self.patch_embed.patch_size[0]\n        h = w = int(x.shape[1]**.5)\n        assert h * w == x.shape[1]\n        \n        x = x.reshape(shape=(x.shape[0], h, w, p, p, 1))\n        x = torch.einsum('nhwpqc->nchpwq', x)\n        imgs = x.reshape(shape=(x.shape[0], 1, h * p, h * p))\n        return imgs\n\n    def random_masking(self, x, mask_ratio):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n        len_keep = int(L * (1 - mask_ratio))\n        \n        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n        \n        # sort noise for each sample\n        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([N, L], device=x.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n\n    def forward_encoder(self, x, mask_ratio):\n        # embed patches\n        x = self.patch_embed(x)\n\n        # add pos embed w/o cls token\n        x = x + self.pos_embed[:, 1:, :]\n\n        # masking: length -> length * mask_ratio\n        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n\n        # append cls token\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x, mask, ids_restore\n\n    def forward_decoder(self, x, ids_restore):\n        # embed tokens\n        x = self.decoder_embed(x)\n\n        # append mask tokens to sequence\n        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n\n        # add pos embed\n        x = x + self.decoder_pos_embed\n\n        # apply Transformer blocks\n        for blk in self.decoder_blocks:\n            x = blk(x)\n        x = self.decoder_norm(x)\n\n        # predictor projection\n        x = self.decoder_pred(x)\n\n        # remove cls token\n        x = x[:, 1:, :]\n\n        return x\n\n    def forward_loss(self, imgs, pred, mask):\n        \"\"\"\n        imgs: [N, 3, H, W]\n        pred: [N, L, p*p*3]\n        mask: [N, L], 0 is keep, 1 is remove, \n        \"\"\"\n        target = self.patchify(imgs)\n        if self.norm_pix_loss:\n            mean = target.mean(dim=-1, keepdim=True)\n            var = target.var(dim=-1, keepdim=True)\n            target = (target - mean) / (var + 1.e-6)**.5\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n\n        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n        return loss\n\n    def forward(self, imgs, mask_ratio=0.75):\n        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n        loss = self.forward_loss(imgs, pred, mask)\n        return loss, pred, mask\n","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.186852Z","iopub.execute_input":"2022-01-19T23:08:32.1872Z","iopub.status.idle":"2022-01-19T23:08:32.226903Z","shell.execute_reply.started":"2022-01-19T23:08:32.187173Z","shell.execute_reply":"2022-01-19T23:08:32.226207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mae = MaskedAutoencoderViT(\n#         patch_size=16, embed_dim=768, depth=12, num_heads=12,\n#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\nmae = MaskedAutoencoderViT(\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n# mae = MaskedAutoencoderViT(\n#         patch_size=16, embed_dim=1280, depth=32, num_heads=16,\n#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:32.22815Z","iopub.execute_input":"2022-01-19T23:08:32.228404Z","iopub.status.idle":"2022-01-19T23:08:36.691293Z","shell.execute_reply.started":"2022-01-19T23:08:32.228347Z","shell.execute_reply":"2022-01-19T23:08:36.69051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae = mae.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:36.692471Z","iopub.execute_input":"2022-01-19T23:08:36.69272Z","iopub.status.idle":"2022-01-19T23:08:41.57204Z","shell.execute_reply.started":"2022-01-19T23:08:36.692688Z","shell.execute_reply":"2022-01-19T23:08:41.571255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# criterion = nn.MSELoss()\n# if torch.cuda.is_available():\n#     criterion = criterion.cuda()\n# optimizer = torch.optim.Adam(mae.parameters(), lr=0.001)\noptimizer = torch.optim.AdamW(mae.parameters(), lr=0.003, betas=(0.9, 0.95))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:41.573203Z","iopub.execute_input":"2022-01-19T23:08:41.573468Z","iopub.status.idle":"2022-01-19T23:08:41.58267Z","shell.execute_reply.started":"2022-01-19T23:08:41.573433Z","shell.execute_reply":"2022-01-19T23:08:41.580936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# if torch.cuda.is_available():\n#     mae = mae.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:41.584009Z","iopub.execute_input":"2022-01-19T23:08:41.584474Z","iopub.status.idle":"2022-01-19T23:08:41.590967Z","shell.execute_reply.started":"2022-01-19T23:08:41.584438Z","shell.execute_reply":"2022-01-19T23:08:41.590224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"!scp ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\npred = np.array\nval_pred = np.array\nloss_list = []\nvloss_list = []\n\nfor epoch in range(800):  # loop over the dataset multiple times\n    i=0\n    running_loss = 0.0\n    val_loss=0.0\n    for data in tqdm(train_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        data = data.to(device)\n        images = data\n        i+=1\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n#         outputs = net(inputs)\n        loss, pred, mask = mae(images)\n#         loss = criterion(pred, masked)\n        loss.backward()\n        optimizer.step()\n        if epoch%50==0:\n#             torch.save(v.state_dict(), './trained-vit'+str(epoch)+'.pt')\n            torch.save(mae, \"./model\"+str(epoch))\n        # print statistics\n        running_loss += loss.item()\n    print('[%d] Training loss: %.5f' %\n            (epoch + 1, running_loss/5))\n    loss_list.append(running_loss)\n    for data in tqdm(val_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        data = data.to(device)\n        images = data\n        i+=1\n#         print(images.shape)\n        val_loss, val_pred,val_mask = mae(images)\n#         print(pred.shape)\n#         loss = criterion(val_pred, val_masked)\n#         if epoch%100==0:\n#             torch.save(v.state_dict(), './trained-vit'+str(epoch)+'.pt')\n#             torch.save(mae, \"./model\"+str(epoch))\n        # print statistics\n        val_loss += loss.item()\n    print('[%d] Validation loss: %.5f' %\n            (epoch + 1, val_loss))\n    vloss_list.append(val_loss)\n\nprint('Finished Training')\n# that's all!\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\n\n# save your improved vision transformer\ntorch.save(mae, './trained-vit')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:08:41.618525Z","iopub.execute_input":"2022-01-19T23:08:41.619031Z","iopub.status.idle":"2022-01-19T23:10:06.308731Z","shell.execute_reply.started":"2022-01-19T23:08:41.618994Z","shell.execute_reply":"2022-01-19T23:10:06.306923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualisation","metadata":{}},{"cell_type":"code","source":"def show_image(image, title=''):\n    # image is [H, W, 3]\n    assert image.shape[2] == 1\n#     plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n    plt.imshow(image)\n    plt.title(title, fontsize=16)\n    plt.axis('off')\n    return\n\ndef run_one_image(img, model):\n    x = torch.tensor(img)\n    x = x.unsqueeze(dim=2)\n    # make it a batch-like\n    x = x.unsqueeze(dim=0)\n    print(x.shape)\n    x = torch.einsum('nhwc->nchw', x)\n\n    # run MAE\n    loss, y, mask = model(x, mask_ratio=0.75)\n    y = model.unpatchify(y)\n    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n\n    # visualize the mask\n    mask = mask.detach()\n    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *1)  # (N, H*W, p*p*3)\n    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n    \n    x = torch.einsum('nchw->nhwc', x)\n\n    # masked image\n    x = x.detach().cpu()\n    im_masked = x * (1 - mask)\n\n    # MAE reconstruction pasted with visible patches\n    im_paste = x * (1 - mask) + y * mask\n\n    # make the plt figure larger\n    plt.rcParams['figure.figsize'] = [24, 24]\n\n    plt.subplot(1, 4, 1)\n    show_image(x[0], \"original\")\n\n    plt.subplot(1, 4, 2)\n    show_image(im_masked[0], \"masked\")\n\n    plt.subplot(1, 4, 3)\n    show_image(y[0], \"reconstruction\")\n\n    plt.subplot(1, 4, 4)\n    show_image(im_paste[0], \"reconstruction + visible\")\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:10:09.592954Z","iopub.execute_input":"2022-01-19T23:10:09.59324Z","iopub.status.idle":"2022-01-19T23:10:09.605243Z","shell.execute_reply.started":"2022-01-19T23:10:09.593208Z","shell.execute_reply":"2022-01-19T23:10:09.60428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nfor x in val_loader:\n    x = x.to(device)\n    run_one_image(x[0][0], mae)\n    i+=1\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:10:12.771031Z","iopub.execute_input":"2022-01-19T23:10:12.771779Z","iopub.status.idle":"2022-01-19T23:10:42.709981Z","shell.execute_reply.started":"2022-01-19T23:10:12.77174Z","shell.execute_reply":"2022-01-19T23:10:42.709313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(len(loss_list)),loss_list, '-')\nplt.plot(range(len(vloss_list)),vloss_list, '-')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T23:10:51.637003Z","iopub.execute_input":"2022-01-19T23:10:51.639264Z","iopub.status.idle":"2022-01-19T23:10:52.027333Z","shell.execute_reply.started":"2022-01-19T23:10:51.63923Z","shell.execute_reply":"2022-01-19T23:10:52.026555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}